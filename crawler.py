# -*- coding: utf-8 -*-
"""
Tving í¬ë¡¤ë§ ëª¨ë“ˆ
"""
import os
import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

def setup_driver():
    """ì›¹ ë“œë¼ì´ë²„ë¥¼ ì„¤ì •í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    return webdriver.Chrome(options=options)

def get_content_links(driver, url):
    """ì£¼ì–´ì§„ URLì—ì„œ ì½˜í…ì¸  ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    driver.get(url)
    time.sleep(5)
    for _ in range(10):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
    cards = driver.find_elements(By.CSS_SELECTOR, "a[href^='/contents/']")
    return list(set(card.get_attribute("href") for card in cards if card.get_attribute("href")))

def get_content_details(driver, link):
    """ê°œë³„ ì½˜í…ì¸  í˜ì´ì§€ì—ì„œ ìƒì„¸ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    driver.get(link)
    
    # ì¸ë„¤ì¼ ì´ë¯¸ì§€ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ìµœëŒ€ 15ì´ˆ ëŒ€ê¸°
    # ëŒ€í‘œ ì´ë¯¸ì§€ëŠ” ë³´í†µ alt ì†ì„±ì„ ê°€ì§€ë¯€ë¡œ, ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒì´ ë” ì•ˆì •ì ì…ë‹ˆë‹¤.
    try:
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "img[alt]"))
        )
    except Exception as e:
        print(f"âš ï¸ ì¸ë„¤ì¼(img[alt]) ë¡œë“œ ëŒ€ê¸° ì¤‘ íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì‹¤íŒ¨í•˜ë”ë¼ë„ ì¼ë‹¨ ì§„í–‰
        pass

    soup = BeautifulSoup(driver.page_source, "html.parser")

    title = "ì œëª© ì—†ìŒ"
    thumbnail = ""

    # âœ… title: alt ì†ì„±ì´ ìˆëŠ” ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œ
    img_tag = soup.select_one("img[alt]")
    if img_tag and img_tag.has_attr("alt"):
        title = img_tag["alt"]

    # âœ… 2. thumbnailì€ ì¡°ê±´ì— ë§ëŠ” ê²ƒë§Œ ë”°ë¡œ íƒìƒ‰
    img_tags = soup.select('img.loaded')
    for img in img_tags:
        src = img.get("src", "")
        if (".jpg" in src or ".jpeg" in src or ".png" in src or ".PNG" in src or ".jfif" in src or ".JPG" in src) and "/resize/480" in src:
            thumbnail = src
            break




    desc_tag = soup.select_one("#__next > main > section > article > article > div.css-1iz9gs3.ee4wkaf4 > div.css-1gc7po1.ee4wkaf5 > p")
    description = desc_tag.get_text(strip=True) if desc_tag else ""

    age_rating = "ì •ë³´ ì—†ìŒ"
    age_div = soup.select_one(".tag-age")
    if age_div:
        for cls in age_div.get("class", []):
            if "tag-age-" in cls:
                code = cls.split("-")[-1]
                age_rating = {
                    "all": "ì „ì²´ ì´ìš©ê°€",
                    "seven": "7ì„¸ ì´ìƒ",
                    "twelve": "12ì„¸ ì´ìƒ",
                    "fifteen": "15ì„¸ ì´ìƒ",
                    "nineteen": "19ì„¸ ì´ìƒ"
                }.get(code, f"ë“±ê¸‰ ë¯¸ìƒ ({code})")
                break

    cast = "ì •ë³´ ì—†ìŒ"
    for dt in soup.find_all("dt"):
        if dt.get_text(strip=True) == "ì¶œì—°":
            dd = dt.find_next_sibling("dd")
            if dd:
                cast = dd.get_text(strip=True)
            break
            
    return {
        "title": title,
        "description": description,
        "thumbnail": thumbnail,
        "age_rating": age_rating,
        "cast": cast,
    }

def save_to_csv(data, genre, subgenre):
    """í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    # ì ˆëŒ€ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ ë””ë ‰í† ë¦¬ ì§€ì •
    base_dir = "C:/work_python/project/crawler/tving_crawler"
    save_dir = os.path.join(base_dir, genre)
    os.makedirs(save_dir, exist_ok=True)
    filename = f"tving_{genre}_{subgenre}.csv"
    filepath = os.path.join(save_dir, filename)

    df = pd.DataFrame(data)
    df.to_csv(filepath, index=False, encoding="utf-8-sig")
    print(f"í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {filepath}")

def crawl_tving_category(genre, subgenre, url):
    """ì§€ì •ëœ ì¹´í…Œê³ ë¦¬ì˜ ëª¨ë“  ì½˜í…ì¸ ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    print(f"ğŸš€ ì‹œì‘: {genre}-{subgenre}")
    driver = setup_driver()
    links = get_content_links(driver, url)
    print(f"ğŸ”— [{genre}-{subgenre}] ì´ {len(links)}ê°œ ë§í¬ ìˆ˜ì§‘")

    data = []
    for idx, link in enumerate(links):
        try:
            details = get_content_details(driver, link)
            details.update({"genre": genre, "subgenre": subgenre})
            data.append(details)
            print(f"[{idx+1}/{len(links)}] {details['title']} ì™„ë£Œ")
        except Exception as e:
            print(f"[{idx+1}] âŒ ì—ëŸ¬: {e}")
            continue
    
    driver.quit()
    save_to_csv(data, genre, subgenre)
